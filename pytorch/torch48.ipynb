{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN\n",
    "\n",
    "# 1 import library\n",
    "import imageio\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# matplotlib style 설정\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "# plt.style.use(\"ggplot\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 variable setting\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "sample_size = 64\n",
    "nz = 128\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 mnist download\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"data2\", train=True, transform=transform, download=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz):\n",
    "        super().__init__()\n",
    "        self.nz = nz\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.nz, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Descriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_input = 784\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.n_input, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2)\n",
      "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2)\n",
      "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
      "    (7): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Dropout(p=0.3, inplace=False)\n",
      "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 6 object setting\n",
    "generator = Generator(nz).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 loss function, optimizer setting\n",
    "optim_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optim_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "losses_g = []\n",
    "losses_d = []\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 image save function\n",
    "def save_generator_image(image, path):\n",
    "    save_image(image, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 descriminator train function\n",
    "def train_discriminator(optimizer, data_real, data_fake):\n",
    "    b_size = data_real.size(0)\n",
    "    real_label = torch.ones(b_size, 1).to(device)\n",
    "    fake_label = torch.zeros(b_size, 1).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output_real = discriminator(data_real)\n",
    "    loss_real = criterion(output_real, real_label)\n",
    "    output_fake = discriminator(data_fake)\n",
    "    loss_fake = criterion(output_fake, fake_label)\n",
    "    loss_real.backward()\n",
    "    loss_fake.backward()\n",
    "    optimizer.step()\n",
    "    return loss_real + loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 generator train function\n",
    "def train_generator(optimizer, data_fake):\n",
    "    b_size = data_fake.size(0)\n",
    "    real_label = torch.ones(b_size, 1).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = discriminator(data_fake)\n",
    "    loss = criterion(output, real_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 200\n",
      "Generator loss: 3.40554142, Discriminator loss: 1.19158053\n",
      "Epoch 1 of 200\n",
      "Generator loss: 2.34370756, Discriminator loss: 0.99131310\n",
      "Epoch 2 of 200\n",
      "Generator loss: 2.33423686, Discriminator loss: 0.78651118\n",
      "Epoch 3 of 200\n",
      "Generator loss: 1.34494126, Discriminator loss: 1.14759362\n",
      "Epoch 4 of 200\n",
      "Generator loss: 3.87813282, Discriminator loss: 0.84375012\n",
      "Epoch 5 of 200\n",
      "Generator loss: 2.74473333, Discriminator loss: 1.02631068\n",
      "Epoch 6 of 200\n",
      "Generator loss: 1.72398365, Discriminator loss: 0.91461843\n",
      "Epoch 7 of 200\n",
      "Generator loss: 2.92544889, Discriminator loss: 1.10974932\n",
      "Epoch 8 of 200\n",
      "Generator loss: 1.82098484, Discriminator loss: 0.92148274\n",
      "Epoch 9 of 200\n",
      "Generator loss: 1.45304501, Discriminator loss: 1.01893234\n",
      "Epoch 10 of 200\n",
      "Generator loss: 1.79675043, Discriminator loss: 0.95732278\n",
      "Epoch 11 of 200\n",
      "Generator loss: 2.06928968, Discriminator loss: 1.13366783\n",
      "Epoch 12 of 200\n",
      "Generator loss: 1.32091761, Discriminator loss: 1.03798187\n",
      "Epoch 13 of 200\n",
      "Generator loss: 2.59357834, Discriminator loss: 0.77576298\n",
      "Epoch 14 of 200\n",
      "Generator loss: 2.28333497, Discriminator loss: 0.95079368\n",
      "Epoch 15 of 200\n",
      "Generator loss: 2.32772827, Discriminator loss: 0.72236371\n",
      "Epoch 16 of 200\n",
      "Generator loss: 2.66101193, Discriminator loss: 0.60529673\n",
      "Epoch 17 of 200\n",
      "Generator loss: 2.70308352, Discriminator loss: 0.84619099\n"
     ]
    }
   ],
   "source": [
    "# 11 model training\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_g = 0.0\n",
    "    loss_d = 0.0\n",
    "    idxh = 0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        image, _ = data\n",
    "        image = image.to(device)\n",
    "        b_size = len(image)\n",
    "        for step in range(k):\n",
    "            data_fake = generator(torch.randn(b_size, nz).to(device).detach())\n",
    "            data_real = image\n",
    "            loss_d += train_discriminator(optim_d, image, data_fake)\n",
    "        data_fake = generator(torch.randn(b_size, nz).to(device).detach())\n",
    "        loss_g += train_generator(optim_g, data_fake)\n",
    "        idxh = idx\n",
    "\n",
    "    generated_image = generator(torch.randn(sample_size, nz).to(device)).cpu().detach()\n",
    "    generated_image = make_grid(generated_image)\n",
    "    save_generator_image(generated_image, f\"data2/generated_images_{epoch}.png\")\n",
    "    images.append(generated_image)\n",
    "    epoch_loss_g = loss_g / idxh\n",
    "    epoch_loss_d = loss_d / idxh\n",
    "    losses_g.append(epoch_loss_g)\n",
    "    losses_d.append(epoch_loss_d)\n",
    "\n",
    "    print(f\"Epoch {epoch} of {epochs}\")\n",
    "    print(f\"Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

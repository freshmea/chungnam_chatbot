{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN\n",
    "\n",
    "# 1 import library\n",
    "import imageio\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# matplotlib style 설정\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "# plt.style.use(\"ggplot\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 variable setting\n",
    "batch_size = 512\n",
    "epochs = 200\n",
    "sample_size = 64\n",
    "nz = 128\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 mnist download\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"data2\", train=True, transform=transform, download=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz):\n",
    "        super().__init__()\n",
    "        self.nz = nz\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.nz, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Descriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_input = 784\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.n_input, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2)\n",
      "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2)\n",
      "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
      "    (7): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Dropout(p=0.3, inplace=False)\n",
      "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 6 object setting\n",
    "generator = Generator(nz).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 loss function, optimizer setting\n",
    "optim_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optim_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "losses_g = []\n",
    "losses_d = []\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 image save function\n",
    "def save_generator_image(image, path):\n",
    "    save_image(image, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 descriminator train function\n",
    "def train_discriminator(optimizer, data_real, data_fake):\n",
    "    b_size = data_real.size(0)\n",
    "    real_label = torch.ones(b_size, 1).to(device)\n",
    "    fake_label = torch.zeros(b_size, 1).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output_real = discriminator(data_real)\n",
    "    loss_real = criterion(output_real, real_label)\n",
    "    output_fake = discriminator(data_fake)\n",
    "    loss_fake = criterion(output_fake, fake_label)\n",
    "    loss_real.backward()\n",
    "    loss_fake.backward()\n",
    "    optimizer.step()\n",
    "    return loss_real + loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 generator train function\n",
    "def train_generator(optimizer, data_fake):\n",
    "    b_size = data_fake.size(0)\n",
    "    real_label = torch.ones(b_size, 1).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = discriminator(data_fake)\n",
    "    loss = criterion(output, real_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 200\n",
      "Generator loss: 3.40554142, Discriminator loss: 1.19158053\n",
      "Epoch 1 of 200\n",
      "Generator loss: 2.34370756, Discriminator loss: 0.99131310\n",
      "Epoch 2 of 200\n",
      "Generator loss: 2.33423686, Discriminator loss: 0.78651118\n",
      "Epoch 3 of 200\n",
      "Generator loss: 1.34494126, Discriminator loss: 1.14759362\n",
      "Epoch 4 of 200\n",
      "Generator loss: 3.87813282, Discriminator loss: 0.84375012\n",
      "Epoch 5 of 200\n",
      "Generator loss: 2.74473333, Discriminator loss: 1.02631068\n",
      "Epoch 6 of 200\n",
      "Generator loss: 1.72398365, Discriminator loss: 0.91461843\n",
      "Epoch 7 of 200\n",
      "Generator loss: 2.92544889, Discriminator loss: 1.10974932\n",
      "Epoch 8 of 200\n",
      "Generator loss: 1.82098484, Discriminator loss: 0.92148274\n",
      "Epoch 9 of 200\n",
      "Generator loss: 1.45304501, Discriminator loss: 1.01893234\n",
      "Epoch 10 of 200\n",
      "Generator loss: 1.79675043, Discriminator loss: 0.95732278\n",
      "Epoch 11 of 200\n",
      "Generator loss: 2.06928968, Discriminator loss: 1.13366783\n",
      "Epoch 12 of 200\n",
      "Generator loss: 1.32091761, Discriminator loss: 1.03798187\n",
      "Epoch 13 of 200\n",
      "Generator loss: 2.59357834, Discriminator loss: 0.77576298\n",
      "Epoch 14 of 200\n",
      "Generator loss: 2.28333497, Discriminator loss: 0.95079368\n",
      "Epoch 15 of 200\n",
      "Generator loss: 2.32772827, Discriminator loss: 0.72236371\n",
      "Epoch 16 of 200\n",
      "Generator loss: 2.66101193, Discriminator loss: 0.60529673\n",
      "Epoch 17 of 200\n",
      "Generator loss: 2.70308352, Discriminator loss: 0.84619099\n",
      "Epoch 18 of 200\n",
      "Generator loss: 2.26156020, Discriminator loss: 0.75971001\n",
      "Epoch 19 of 200\n",
      "Generator loss: 2.45603466, Discriminator loss: 0.79074150\n",
      "Epoch 20 of 200\n",
      "Generator loss: 3.05583763, Discriminator loss: 0.60461169\n",
      "Epoch 21 of 200\n",
      "Generator loss: 3.44908547, Discriminator loss: 0.51526886\n",
      "Epoch 22 of 200\n",
      "Generator loss: 2.76988649, Discriminator loss: 0.63640255\n",
      "Epoch 23 of 200\n",
      "Generator loss: 3.12357473, Discriminator loss: 0.53982490\n",
      "Epoch 24 of 200\n",
      "Generator loss: 2.32902455, Discriminator loss: 0.74261755\n",
      "Epoch 25 of 200\n",
      "Generator loss: 2.85993409, Discriminator loss: 0.40142080\n",
      "Epoch 26 of 200\n",
      "Generator loss: 2.80540633, Discriminator loss: 0.50530308\n",
      "Epoch 27 of 200\n",
      "Generator loss: 2.71971869, Discriminator loss: 0.56903762\n",
      "Epoch 28 of 200\n",
      "Generator loss: 2.79297304, Discriminator loss: 0.58077282\n",
      "Epoch 29 of 200\n",
      "Generator loss: 2.41759086, Discriminator loss: 0.61575752\n",
      "Epoch 30 of 200\n",
      "Generator loss: 2.32751298, Discriminator loss: 0.57306689\n",
      "Epoch 31 of 200\n",
      "Generator loss: 2.45269537, Discriminator loss: 0.62058491\n",
      "Epoch 32 of 200\n",
      "Generator loss: 2.67401719, Discriminator loss: 0.47884676\n",
      "Epoch 33 of 200\n",
      "Generator loss: 2.66544771, Discriminator loss: 0.56526172\n",
      "Epoch 34 of 200\n",
      "Generator loss: 2.57981300, Discriminator loss: 0.61887527\n",
      "Epoch 35 of 200\n",
      "Generator loss: 2.45656180, Discriminator loss: 0.60915828\n",
      "Epoch 36 of 200\n",
      "Generator loss: 2.55246067, Discriminator loss: 0.57503569\n",
      "Epoch 37 of 200\n",
      "Generator loss: 2.34196877, Discriminator loss: 0.70533711\n",
      "Epoch 38 of 200\n",
      "Generator loss: 2.71261907, Discriminator loss: 0.58221215\n",
      "Epoch 39 of 200\n",
      "Generator loss: 2.84750772, Discriminator loss: 0.51859593\n",
      "Epoch 40 of 200\n",
      "Generator loss: 2.93912792, Discriminator loss: 0.48864156\n",
      "Epoch 41 of 200\n",
      "Generator loss: 2.91261077, Discriminator loss: 0.48283532\n",
      "Epoch 42 of 200\n",
      "Generator loss: 2.66309881, Discriminator loss: 0.55518401\n",
      "Epoch 43 of 200\n",
      "Generator loss: 2.51392913, Discriminator loss: 0.66715360\n",
      "Epoch 44 of 200\n",
      "Generator loss: 2.70995831, Discriminator loss: 0.51114911\n",
      "Epoch 45 of 200\n",
      "Generator loss: 2.92548752, Discriminator loss: 0.47295013\n",
      "Epoch 46 of 200\n",
      "Generator loss: 2.85735178, Discriminator loss: 0.56075561\n",
      "Epoch 47 of 200\n",
      "Generator loss: 2.87943220, Discriminator loss: 0.57253671\n",
      "Epoch 48 of 200\n",
      "Generator loss: 2.72714591, Discriminator loss: 0.55869472\n",
      "Epoch 49 of 200\n",
      "Generator loss: 2.87735677, Discriminator loss: 0.58370262\n",
      "Epoch 50 of 200\n",
      "Generator loss: 2.29101419, Discriminator loss: 0.74311328\n",
      "Epoch 51 of 200\n",
      "Generator loss: 2.52586102, Discriminator loss: 0.56428128\n",
      "Epoch 52 of 200\n",
      "Generator loss: 2.69906878, Discriminator loss: 0.58739948\n",
      "Epoch 53 of 200\n",
      "Generator loss: 2.60069513, Discriminator loss: 0.60010684\n",
      "Epoch 54 of 200\n",
      "Generator loss: 2.66911983, Discriminator loss: 0.57126284\n",
      "Epoch 55 of 200\n",
      "Generator loss: 2.55197263, Discriminator loss: 0.57950610\n",
      "Epoch 56 of 200\n",
      "Generator loss: 2.50359893, Discriminator loss: 0.66005701\n",
      "Epoch 57 of 200\n",
      "Generator loss: 2.39112306, Discriminator loss: 0.70236510\n",
      "Epoch 58 of 200\n",
      "Generator loss: 2.46882153, Discriminator loss: 0.66394210\n",
      "Epoch 59 of 200\n",
      "Generator loss: 2.58165765, Discriminator loss: 0.63685936\n",
      "Epoch 60 of 200\n",
      "Generator loss: 2.56272316, Discriminator loss: 0.63064331\n",
      "Epoch 61 of 200\n",
      "Generator loss: 2.42113018, Discriminator loss: 0.67508823\n",
      "Epoch 62 of 200\n",
      "Generator loss: 2.30628777, Discriminator loss: 0.70240986\n",
      "Epoch 63 of 200\n",
      "Generator loss: 2.35985303, Discriminator loss: 0.66484433\n",
      "Epoch 64 of 200\n",
      "Generator loss: 2.53560781, Discriminator loss: 0.61007738\n",
      "Epoch 65 of 200\n",
      "Generator loss: 2.38241839, Discriminator loss: 0.68261111\n",
      "Epoch 66 of 200\n",
      "Generator loss: 2.37168956, Discriminator loss: 0.73258460\n",
      "Epoch 67 of 200\n",
      "Generator loss: 2.33409476, Discriminator loss: 0.66275018\n",
      "Epoch 68 of 200\n",
      "Generator loss: 2.39241242, Discriminator loss: 0.66089833\n",
      "Epoch 69 of 200\n",
      "Generator loss: 2.32614827, Discriminator loss: 0.64543033\n"
     ]
    }
   ],
   "source": [
    "# 11 model training\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_g = 0.0\n",
    "    loss_d = 0.0\n",
    "    idxh = 0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        image, _ = data\n",
    "        image = image.to(device)\n",
    "        b_size = len(image)\n",
    "        for step in range(k):\n",
    "            data_fake = generator(torch.randn(b_size, nz).to(device).detach())\n",
    "            data_real = image\n",
    "            loss_d += train_discriminator(optim_d, image, data_fake)\n",
    "        data_fake = generator(torch.randn(b_size, nz).to(device).detach())\n",
    "        loss_g += train_generator(optim_g, data_fake)\n",
    "        idxh = idx\n",
    "\n",
    "    generated_image = generator(torch.randn(sample_size, nz).to(device)).cpu().detach()\n",
    "    generated_image = make_grid(generated_image)\n",
    "    save_generator_image(generated_image, f\"data2/generated_images_{epoch}.png\")\n",
    "    images.append(generated_image)\n",
    "    epoch_loss_g = loss_g / idxh\n",
    "    epoch_loss_d = loss_d / idxh\n",
    "    losses_g.append(epoch_loss_g)\n",
    "    losses_d.append(epoch_loss_d)\n",
    "\n",
    "    print(f\"Epoch {epoch} of {epochs}\")\n",
    "    print(f\"Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
